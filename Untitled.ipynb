{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "middle-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import count, chain\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "train_file = \"data/train1.oracle\"\n",
    "dev_file = \"data/dev1.oracle\"\n",
    "test_file = \"data/test1.oracle\"\n",
    "cluster_file = \"data/bllip-clusters\"\n",
    "\n",
    "WORD_DIM = 50\n",
    "LSTM_DIM = 256 # same for input and hidden layers\n",
    "ACTION_DIM = 16 # dimension for action embedding\n",
    "\n",
    "class Vocab:\n",
    "  def __init__(self, w2i):\n",
    "    self.w2i = dict(w2i)\n",
    "    self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "  @classmethod\n",
    "  def from_list(cls, words):\n",
    "    w2i = {}\n",
    "    idx = 0\n",
    "    for word in words:\n",
    "      w2i[word] = idx\n",
    "      idx += 1\n",
    "    return Vocab(w2i)\n",
    "\n",
    "  @classmethod\n",
    "  def from_file(cls, vocab_fname):\n",
    "    words = []\n",
    "    with open(vocab_fname) as fh:\n",
    "      for line in fh:\n",
    "        line.strip()\n",
    "        cluster, word, count = line.split(\"\\t\")\n",
    "        words.append(word)\n",
    "    return Vocab.from_list(words)  \n",
    "\n",
    "  def merge_vocab(self, dic):\n",
    "    self.w2i = Vocab(self.w2i.keys() + dic.keys()).w2i\n",
    "    self.i2w = {i:w for w,i in self.w2i.items()}\n",
    "\n",
    "  def size(self): return len(self.w2i.keys())\n",
    "\n",
    "class TransitionParser:\n",
    "  def __init__(self, model, cluster_filepath, vocab_acts, WORD_DIM=50, LSTM_DIM=256, ACTION_DIM=16):\n",
    "    self.vocab = Vocab.from_file(cluster_filepath)\n",
    "    print(\"bob0\")\n",
    "    #self.vocab.w2i[\"UNK\"] = self.vocab.size() + 1 # add \"UNK\" to vocabulary... nope =/\n",
    "    self.vocab_acts = vocab_acts\n",
    "    self.vocab_NTs = Vocab.from_list(get_NTs(vocab_acts.w2i.keys()))\n",
    "    self.act_NT_map = dict([[vocab_acts.w2i[x], self.vocab_NTs.w2i[x[3:-1]]] \n",
    "                            for x in vocab_acts.w2i if x.startswith(\"NT\")])\n",
    "    print(\"bob\")\n",
    "\n",
    "    # parameters\n",
    "    self.pW_comp = model.add_parameters((LSTM_DIM, LSTM_DIM*2))\n",
    "    self.pb_comp = model.add_parameters((LSTM_DIM, ))\n",
    "    self.pW_s2h = model.add_parameters((LSTM_DIM, LSTM_DIM ))\n",
    "    self.pb_s2h = model.add_parameters((LSTM_DIM, ))\n",
    "    self.pW_act = model.add_parameters((vocab_acts.size(), LSTM_DIM))\n",
    "    self.pb_act = model.add_parameters((vocab_acts.size(), ))\n",
    "    self.pempty_stack_emb = model.add_parameters((LSTM_DIM,)) # empty stack embedding /root guard\n",
    "    print(\"bob2\")\n",
    "    # layers, in-dim, out-dim, model\n",
    "    self.stackRNN = dy.CoupledLSTMBuilder(2, LSTM_DIM, LSTM_DIM, model)\n",
    "    self.comp_LSTM_fwd = dy.CoupledLSTMBuilder(2, LSTM_DIM, LSTM_DIM, model)\n",
    "    self.comp_LSTM_rev = dy.CoupledLSTMBuilder(2, LSTM_DIM, LSTM_DIM, model)\n",
    "    self.cfsm = dy.ClassFactoredSoftmaxBuilder(LSTM_DIM, cluster_filepath, self.vocab.w2i, model);\n",
    "    print(\"bob3\")\n",
    "    # lookup params\n",
    "    self.WORDS_LOOKUP = model.add_lookup_parameters((self.vocab.size(), LSTM_DIM))\n",
    "    self.ACT_LOOKUP = model.add_lookup_parameters((self.vocab_acts.size(), ACTION_DIM))\n",
    "    self.NT_LOOKUP = model.add_lookup_parameters((self.vocab_NTs.size(), LSTM_DIM))\n",
    "    print(\"bob4\")\n",
    "    self.pc = model\n",
    "\n",
    "  def gen_setup(self, dropout=None):\n",
    "    dy.renew_cg()\n",
    "    stack = []\n",
    "    stack.append((self.stackRNN.initial_state().add_input(dy.parameter(self.pempty_stack_emb)), \n",
    "      \"<ROOT GUARD>\")) # stack holds tuples: RNN state, string rep\n",
    "    W_comp = dy.parameter(self.pW_comp)\n",
    "    b_comp = dy.parameter(self.pb_comp)\n",
    "    W_s2h = dy.parameter(self.pW_s2h)\n",
    "    b_s2h = dy.parameter(self.pb_s2h)\n",
    "    W_act = dy.parameter(self.pW_act)\n",
    "    b_act = dy.parameter(self.pb_act)\n",
    "    if dropout:\n",
    "      self.stackRNN.set_dropout(dropout)\n",
    "      self.comp_LSTM_fwd.set_dropout(dropout)\n",
    "      self.comp_LSTM_rev.set_dropout(dropout)\n",
    "    else:\n",
    "      self.stackRNN.disable_dropout()\n",
    "      self.comp_LSTM_fwd.disable_dropout()\n",
    "      self.comp_LSTM_rev.disable_dropout()\n",
    "    return [stack, # initial stack state,\n",
    "            {\"comp\": (b_comp, W_comp), # bias and weight params for composition\n",
    "              \"s2h\": (b_s2h, W_s2h), # bias and weight params for parser state\n",
    "              \"act\": (b_act, W_act)}] # bias and weight params for predicting actions \n",
    "\n",
    "  def get_valid_actions(self, stack, open_nts, open_nt_ceil=100):\n",
    "    # based on stack state, get valid actions\n",
    "    valid_actions = []\n",
    "    n_open_nts = len(open_nts)\n",
    "    if n_open_nts < open_nt_ceil:  \n",
    "      valid_actions += [v for k, v in self.vocab_acts.w2i.items() if k.startswith(\"NT\")]\n",
    "    if n_open_nts >= 1 and len(stack) > 1:\n",
    "      valid_actions += [self.vocab_acts.w2i[\"SHIFT\"]]\n",
    "    if n_open_nts >= 1  and len(stack) > 1 \\\n",
    "        and len(stack) - 1 > open_nts[-1]:  # top element on stack can't be open NT\n",
    "      valid_actions += [self.vocab_acts.w2i[\"REDUCE\"]]\n",
    "    return valid_actions\n",
    "\n",
    "  def predict_action(self, stack, params, valid_actions, dropout=None):\n",
    "    stack_embedding = stack[-1][0].output() \n",
    "    if dropout:\n",
    "      stack_embedding = dy.dropout(stack_embedding, dropout)\n",
    "    parser_state = dy.rectify(dy.affine_transform([params[\"s2h\"][0], params[\"s2h\"][1], stack_embedding]))\n",
    "    logits = dy.affine_transform([params[\"act\"][0], params[\"act\"][1], parser_state])\n",
    "    log_probs = dy.log_softmax(logits, valid_actions)\n",
    "    return log_probs \n",
    "\n",
    "  def get_action(self, stack, params, valid_actions, n_actions, train_acts=None, dropout=None):\n",
    "    action = valid_actions[0]\n",
    "    loss = None\n",
    "    if len(valid_actions) > 1:\n",
    "      log_probs = self.predict_action(stack, params, valid_actions, dropout)\n",
    "      if train_acts:\n",
    "        try:\n",
    "          action = self.vocab_acts.w2i[train_acts[n_actions]]\n",
    "        except IndexError:\n",
    "          raise Exception(\"Correct action list exhausted, but not in final parser state.\")\n",
    "        loss = dy.pick(log_probs, action)\n",
    "      else:\n",
    "        action = max(enumerate(log_probs.vec_value()), key=itemgetter(1))[0]\n",
    "    return action, loss\n",
    "\n",
    "  def do_action(self, stack, action, params, open_nts, n_terms, train_sent=None, dropout=None):\n",
    "    if action in self.vocab_acts.i2w:\n",
    "      act = action\n",
    "      action = self.vocab_acts.i2w[action]\n",
    "    else:\n",
    "      act = self.vocab_acts.w2i[action]\n",
    "    word, nt_index, loss = \"\", 0, None\n",
    "    if action == \"SHIFT\":\n",
    "      if train_sent:\n",
    "        try:\n",
    "          word = train_sent[n_terms]\n",
    "        except IndexError:\n",
    "          # raise Exception(\"Generated more terms than found in training sentence\")\n",
    "          pass\n",
    "        if word not in self.vocab.w2i: ### for now, treat clusters as vocab\n",
    "          if word.lower() in self.vocab.w2i:\n",
    "            word = word.lower()\n",
    "          else:\n",
    "            #word = \"UNK\" ### - all words not in cluster file get UNKified - OR...\n",
    "            word = random.sample(self.vocab.w2i.keys(), 1)[0] # ...replaced w/rando in-vocab wd\n",
    "            # TODO: ^ this is not at all optimal, figure out how to fix\n",
    "        loss = -self.cfsm.neg_log_softmax(stack[-1][0].output(), self.vocab.w2i[word])\n",
    "      else:\n",
    "        word = self.vocab.i2w[self.cfsm.sample(stack[-1][0].output())]\n",
    "      word_embedding = self.WORDS_LOOKUP[self.vocab.w2i[word]]\n",
    "      stack.append((stack[-1][0].add_input(word_embedding), word))\n",
    "    elif action == \"REDUCE\":\n",
    "      children = []\n",
    "      last_nt_idx = open_nts.pop()\n",
    "      while len(stack) > last_nt_idx + 1:\n",
    "        children.append(stack.pop()) \n",
    "      children.reverse()\n",
    "      last_nt = stack.pop()\n",
    "      fwd = self.comp_LSTM_fwd.initial_state().add_input(last_nt[0].output())\n",
    "      rev = self.comp_LSTM_rev.initial_state().add_input(last_nt[0].output())\n",
    "      for i, child in enumerate(children):\n",
    "        fwd.add_input(child[0].output())\n",
    "        rev.add_input(children[len(children) - i - 1][0].output())\n",
    "      cfwd = dy.dropout(fwd.output(), dropout) if dropout else fwd.output()\n",
    "      crev = dy.dropout(rev.output(), dropout) if dropout else rev.output()\n",
    "      bidir_rep = dy.concatenate([cfwd, crev])\n",
    "      composed = dy.rectify(dy.affine_transform([params[\"comp\"][0], \n",
    "                                                params[\"comp\"][1], bidir_rep]))\n",
    "      comp_str = last_nt[1] + \" \" + \" \".join([child[1] for child in children]) + \")\"\n",
    "      stack.append((stack[-1][0].add_input(composed), comp_str))\n",
    "    else: # open nonterminal\n",
    "      NT = self.act_NT_map[act]\n",
    "      nt_embedding = self.NT_LOOKUP[NT]\n",
    "      stack.append((stack[-1][0].add_input(nt_embedding), \"(\"+self.vocab_NTs.i2w[NT]))\n",
    "      nt_index = len(stack) - 1\n",
    "    return word, nt_index, loss\n",
    "\n",
    "  def generate(self, train_sent=None, train_acts=None, dropout=None, nt_ceil=100):\n",
    "    stack, params = self.gen_setup(dropout)\n",
    "    terms, actions, open_nts, losses = [], [], [], []\n",
    "    while len(terms) == 0 or len(stack) > 2 :\n",
    "      valid_actions = self.get_valid_actions(stack, open_nts, nt_ceil)\n",
    "      action, loss = self.get_action(stack, params, valid_actions, len(actions), train_acts, dropout)\n",
    "      losses.append(loss) if loss else loss\n",
    "      actions.append(action)\n",
    "      term, nt_idx, loss = self.do_action(stack, action, params, open_nts, len(terms), train_sent, dropout)\n",
    "      losses.append(loss) if loss else loss\n",
    "      terms.append(term) if term else term\n",
    "      open_nts.append(nt_idx) if nt_idx else nt_idx # open NT index is never 0\n",
    "      if not train_sent:\n",
    "        if len(terms) % 5 == 0:\n",
    "          print(\" \".join(terms))\n",
    "    final_tree = stack[1][1]\n",
    "    return final_tree, -dy.esum(losses) if losses else None\n",
    "\n",
    "  def test(self, test_set):\n",
    "    test_set.sort(key=lambda x: len(x[1]))\n",
    "    corpus = [test_set[i] for i in range(len(test_set))]\n",
    "    total_loss = 0.0\n",
    "    words = 0\n",
    "    i = 0\n",
    "    for (_, s, a) in corpus:\n",
    "      i += 1\n",
    "      result, loss = self.generate(s, a)\n",
    "      words += len(s)\n",
    "      if loss is not None:\n",
    "        total_loss += loss.scalar_value()\n",
    "      if i % 50 == 0:\n",
    "        print(total_loss / words)\n",
    "        total_loss, words = 0, 0\n",
    "\n",
    "  def train(self, corpus, trainer, dev=None, dropout=.3, epochs=3, max_iter=None):\n",
    "    i = 0\n",
    "    losses_graph = []\n",
    "    results_store = []\n",
    "    corpus.sort(key=lambda x: len(x[1])) \n",
    "    order = list(range(len(corpus)))\n",
    "    for epoch in range(epochs):\n",
    "      random.shuffle(order)\n",
    "      shuffled_corpus = [corpus[i] for i in order]\n",
    "      words = 0\n",
    "      total_loss = 0.0\n",
    "      for (_, s,a) in shuffled_corpus:\n",
    "        result, loss = self.generate(s, a, dropout)\n",
    "        words += len(s)\n",
    "        if loss is not None:\n",
    "          total_loss += loss.scalar_value()\n",
    "          loss.backward()\n",
    "          trainer.update()\n",
    "        e = float(i) / len(corpus)\n",
    "        if i % 25 == 0:\n",
    "          losses_graph.append(total_loss / words)\n",
    "          print('epoch {}: per-word loss: {}'.format(e, total_loss / words))\n",
    "          if e > 1:\n",
    "           result, loss= self.generate()\n",
    "           print(\"   {}\".format(result))\n",
    "           results_store.append(result)\n",
    "          words = 0\n",
    "          total_loss = 0.0\n",
    "        # if i % 500 == 0 and dev:\n",
    "          # dev_words = 0\n",
    "          # dev_loss = 0.0\n",
    "          # for (_, ds, da) in dev:\n",
    "            # result, loss = self.generate(ds, da)\n",
    "            # dev_words += len(ds)\n",
    "            # if loss is not None:\n",
    "              # dev_loss += loss.scalar_value()\n",
    "          # print('[validation] epoch {}: per-word loss: {}'.format(e, dev_loss / dev_words))\n",
    "        i += 1\n",
    "        if max_iter:\n",
    "          if i >= max_iter:\n",
    "            break\n",
    "      if max_iter:\n",
    "        if i >= max_iter:\n",
    "          break\n",
    "    return losses_graph, results_store\n",
    "\n",
    "def read_oracle(fname, gen=True):\n",
    "  sent_idx = 1 if gen else 4 # using non-UNKified sentences\n",
    "  act_idx = 3 if gen else 5\n",
    "  with open(fname) as fh:\n",
    "    sent_ctr = 0\n",
    "    tree, sent, acts = \"\", [], []\n",
    "    for line in fh:\n",
    "      sent_ctr += 1\n",
    "      line = line.strip()\n",
    "      if line.startswith(\"#\"):\n",
    "        sent_ctr = 0\n",
    "        if tree:\n",
    "          yield tree, sent, acts\n",
    "        tree, sent, acts = line, [], []\n",
    "      if sent_ctr == sent_idx:\n",
    "        sent = line.split()\n",
    "      if sent_ctr >= act_idx:\n",
    "        if line:\n",
    "          acts.append(line)\n",
    "\n",
    "def load_data(tr=train_file, d=dev_file, ts=test_file):\n",
    "  train, dev, test = [], [], []\n",
    "  if tr:\n",
    "    train = list(read_oracle(tr))\n",
    "  if d:\n",
    "    dev = list(read_oracle(d))\n",
    "  if ts:\n",
    "    test = list(read_oracle(ts))\n",
    "  return train, dev, test\n",
    "\n",
    "def create_vocab(all_terms):\n",
    "  vocab = list(set(list(chain(*all_terms))))\n",
    "  return Vocab.from_list(vocab)\n",
    "\n",
    "def get_NTs(actions):\n",
    "  NTs = []\n",
    "  for act in actions:\n",
    "    if act.startswith(\"NT\"):\n",
    "      NTs.append(act[3:-1])\n",
    "  return NTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "passive-chess",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets go\n",
      "done loading files\n",
      "loading base model params\n",
      "done loading param graph\n",
      "bob0\n",
      "bob\n",
      "bob2\n",
      "bob3\n",
      "bob4\n",
      "loading model\n",
      "done loading model\n",
      "epoch 0.0: per-word loss: 3.4422242641448975\n",
      "epoch 0.010803802938634399: per-word loss: 3.47006719074552\n",
      "epoch 0.021607605877268798: per-word loss: 3.599647293027663\n",
      "epoch 0.032411408815903195: per-word loss: 3.9015098839998243\n",
      "epoch 0.043215211754537596: per-word loss: 3.9874326501573836\n",
      "epoch 0.054019014693171996: per-word loss: 3.5470473098754884\n",
      "epoch 0.06482281763180639: per-word loss: 3.2300968264278613\n",
      "epoch 0.0756266205704408: per-word loss: 3.582009514436027\n",
      "epoch 0.08643042350907519: per-word loss: 3.643854120844289\n",
      "epoch 0.0972342264477096: per-word loss: 3.986881376195837\n",
      "epoch 0.10803802938634399: per-word loss: 3.7850933288460347\n",
      "epoch 0.11884183232497839: per-word loss: 3.68049428126956\n",
      "epoch 0.12964563526361278: per-word loss: 3.8781651706560285\n",
      "epoch 0.1404494382022472: per-word loss: 2.824750065803528\n",
      "epoch 0.1512532411408816: per-word loss: 4.017046373207252\n",
      "epoch 0.162057044079516: per-word loss: 2.7262334480124006\n",
      "epoch 0.17286084701815038: per-word loss: 3.3407157812799726\n",
      "epoch 0.18366464995678478: per-word loss: 3.8042033802379263\n",
      "epoch 0.1944684528954192: per-word loss: 4.271933389015687\n",
      "epoch 0.2052722558340536: per-word loss: 3.5846750456711343\n",
      "epoch 0.21607605877268798: per-word loss: 4.33113034680593\n",
      "epoch 0.22687986171132238: per-word loss: 3.8565493037429035\n",
      "epoch 0.23768366464995677: per-word loss: 3.0570366924459282\n",
      "epoch 0.2484874675885912: per-word loss: 3.9016680702841353\n",
      "epoch 0.25929127052722556: per-word loss: 3.409250037114423\n",
      "epoch 0.27009507346586: per-word loss: 4.114168158475904\n",
      "epoch 0.2808988764044944: per-word loss: 3.664382481037226\n",
      "epoch 0.29170267934312877: per-word loss: 3.4486085627530074\n",
      "epoch 0.3025064822817632: per-word loss: 3.220987765412582\n",
      "epoch 0.31331028522039756: per-word loss: 3.60854443194161\n",
      "epoch 0.324114088159032: per-word loss: 3.959960464221328\n",
      "epoch 0.3349178910976664: per-word loss: 3.724479206565286\n",
      "epoch 0.34572169403630076: per-word loss: 3.294669425940212\n",
      "epoch 0.3565254969749352: per-word loss: 2.9329342714092075\n",
      "epoch 0.36732929991356955: per-word loss: 4.410916097702518\n",
      "epoch 0.378133102852204: per-word loss: 3.5033290882905326\n",
      "epoch 0.3889369057908384: per-word loss: 3.8758887488266516\n",
      "epoch 0.39974070872947276: per-word loss: 3.858473304199846\n",
      "epoch 0.4105445116681072: per-word loss: 3.0976511811007974\n",
      "epoch 0.42134831460674155: per-word loss: 2.960852632586588\n",
      "epoch 0.43215211754537597: per-word loss: 3.6965211037989265\n",
      "epoch 0.4429559204840104: per-word loss: 2.497726369673206\n",
      "epoch 0.45375972342264476: per-word loss: 3.778422083591391\n",
      "epoch 0.4645635263612792: per-word loss: 2.822954993247986\n",
      "epoch 0.47536732929991354: per-word loss: 4.1295162374026155\n",
      "epoch 0.48617113223854796: per-word loss: 3.004651388112646\n",
      "epoch 0.4969749351771824: per-word loss: 4.080164651185768\n",
      "epoch 0.5077787381158168: per-word loss: 2.618384945924115\n",
      "epoch 0.5185825410544511: per-word loss: 3.843007792745318\n",
      "epoch 0.5293863439930856: per-word loss: 3.92074696163633\n",
      "epoch 0.54019014693172: per-word loss: 3.087774462170071\n",
      "epoch 0.5509939498703543: per-word loss: 3.4409637279647716\n",
      "epoch 0.5617977528089888: per-word loss: 3.7284951134333535\n",
      "epoch 0.5726015557476232: per-word loss: 3.5424115494506\n",
      "epoch 0.5834053586862575: per-word loss: 3.997362502415975\n",
      "epoch 0.594209161624892: per-word loss: 3.920258427319461\n",
      "epoch 0.6050129645635264: per-word loss: 3.8475544744258303\n",
      "epoch 0.6158167675021607: per-word loss: 3.9087682637301358\n",
      "epoch 0.6266205704407951: per-word loss: 3.594586621312534\n",
      "epoch 0.6374243733794296: per-word loss: 3.6927563419824914\n",
      "epoch 0.648228176318064: per-word loss: 3.306808528521203\n",
      "epoch 0.6590319792566983: per-word loss: 3.2092748318078383\n",
      "epoch 0.6698357821953328: per-word loss: 3.0644976389493874\n",
      "epoch 0.6806395851339672: per-word loss: 3.480475713224972\n",
      "epoch 0.6914433880726015: per-word loss: 3.5892142924395474\n",
      "epoch 0.702247191011236: per-word loss: 3.666656685202089\n",
      "epoch 0.7130509939498704: per-word loss: 3.539074943179176\n",
      "epoch 0.7238547968885047: per-word loss: 3.3962484705234\n",
      "epoch 0.7346585998271391: per-word loss: 4.20533702547187\n",
      "epoch 0.7454624027657736: per-word loss: 4.109046639863008\n",
      "epoch 0.756266205704408: per-word loss: 4.10751465764539\n",
      "epoch 0.7670700086430423: per-word loss: 3.956614038612269\n",
      "epoch 0.7778738115816768: per-word loss: 3.5141405393095577\n",
      "epoch 0.7886776145203112: per-word loss: 3.5978503101012285\n",
      "epoch 0.7994814174589455: per-word loss: 3.052660919004871\n",
      "epoch 0.81028522039758: per-word loss: 3.618855949882027\n",
      "epoch 0.8210890233362144: per-word loss: 4.014275795883602\n",
      "epoch 0.8318928262748487: per-word loss: 3.651511499847191\n",
      "epoch 0.8426966292134831: per-word loss: 3.0355941513005424\n",
      "epoch 0.8535004321521176: per-word loss: 3.768723490834236\n",
      "epoch 0.8643042350907519: per-word loss: 3.957059891687499\n",
      "epoch 0.8751080380293863: per-word loss: 2.830167683688077\n",
      "epoch 0.8859118409680208: per-word loss: 3.179273346801857\n",
      "epoch 0.8967156439066551: per-word loss: 3.6873631337109734\n",
      "epoch 0.9075194468452895: per-word loss: 3.6037408835451368\n",
      "epoch 0.918323249783924: per-word loss: 3.122245801612735\n",
      "epoch 0.9291270527225584: per-word loss: 3.417932117222161\n",
      "epoch 0.9399308556611927: per-word loss: 4.306057296260711\n",
      "epoch 0.9507346585998271: per-word loss: 3.3928253650665283\n",
      "epoch 0.9615384615384616: per-word loss: 4.33597354768957\n",
      "epoch 0.9723422644770959: per-word loss: 3.37210472571997\n",
      "epoch 0.9831460674157303: per-word loss: 3.603920201997499\n",
      "epoch 0.9939498703543648: per-word loss: 4.112675906708522\n",
      "epoch 1.0047536732929991: per-word loss: 3.050766185178595\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 1.0155574762316335: per-word loss: 4.3637807386892815\n",
      "\n",
      "\n",
      "   (FRAG (NP pipe) .)\n",
      "epoch 1.0263612791702679: per-word loss: 3.573961084171877\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.0371650821089022: per-word loss: 3.376491652594672\n",
      "\n",
      "\n",
      "   (FRAG (NP elevator) .)\n",
      "epoch 1.0479688850475368: per-word loss: 3.171206058376897\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.0587726879861712: per-word loss: 3.874794307028412\n",
      "\n",
      "\n",
      "you do n't know .\n",
      "you do n't know .\n",
      "   (S (NP you) (VP do n't (VP know)) .)\n",
      "epoch 1.0695764909248056: per-word loss: 3.5500244371818774\n",
      "\n",
      "   (INTJ oh no)\n",
      "epoch 1.08038029386344: per-word loss: 2.911850757598877\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 1.0911840968020743: per-word loss: 3.66585729586198\n",
      "\n",
      "\n",
      "   (S (NP he) (VP 's (ADJP right)) .)\n",
      "epoch 1.1019878997407087: per-word loss: 4.572780081944436\n",
      "\n",
      "\n",
      "   (FRAG (NP you) .)\n",
      "epoch 1.112791702679343: per-word loss: 2.627832167076342\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.1235955056179776: per-word loss: 2.8807981862317797\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.134399308556612: per-word loss: 2.9494229485006893\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.1452031114952463: per-word loss: 4.080800286766623\n",
      "\n",
      "\n",
      "   (FRAG (NP Mommy) .)\n",
      "epoch 1.1560069144338807: per-word loss: 3.2725572727344656\n",
      "\n",
      "\n",
      "   (FRAG (WHNP what) ?)\n",
      "epoch 1.166810717372515: per-word loss: 3.155789638732697\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.1776145203111494: per-word loss: 3.7221574584643045\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.188418323249784: per-word loss: 3.570701623367051\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.1992221261884184: per-word loss: 4.344724627306862\n",
      "\n",
      "   (INTJ ok .)\n",
      "epoch 1.2100259291270528: per-word loss: 4.341984344691765\n",
      "\n",
      "\n",
      "this are crossing the box\n",
      "this are crossing the box\n",
      "this are crossing the box\n",
      "this are crossing the box\n",
      "   (S (NP this) (VP are (VP crossing (NP the box))) ?)\n",
      "epoch 1.2208297320656871: per-word loss: 2.763315718873103\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.2316335350043215: per-word loss: 3.425968493063619\n",
      "\n",
      "   (INTJ no ?)\n",
      "epoch 1.2424373379429559: per-word loss: 3.1548727716718403\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 1.2532411408815902: per-word loss: 3.7913914438503893\n",
      "\n",
      "   (INTJ ?)\n",
      "epoch 1.2640449438202248: per-word loss: 4.694350541003643\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.2748487467588592: per-word loss: 3.795222661266588\n",
      "\n",
      "\n",
      "   (FRAG (NP Adam) .)\n",
      "epoch 1.2856525496974935: per-word loss: 3.114927221660965\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.296456352636128: per-word loss: 3.5843183233382856\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.3072601555747623: per-word loss: 3.839085828873419\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.3180639585133966: per-word loss: 4.12238135843566\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.328867761452031: per-word loss: 2.8461393390012826\n",
      "\n",
      "   (INTJ much-sullied)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.3396715643906656: per-word loss: 4.250650857855206\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.3504753673293: per-word loss: 3.438610989706857\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.3612791702679343: per-word loss: 3.672200613473294\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.3720829732065687: per-word loss: 3.3748973205055988\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.382886776145203: per-word loss: 3.4483617598360237\n",
      "\n",
      "   (INTJ oh no .)\n",
      "epoch 1.3936905790838374: per-word loss: 3.31462662092602\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.404494382022472: per-word loss: 3.2403033586937613\n",
      "\n",
      "   (INTJ hmm .)\n",
      "epoch 1.4152981849611064: per-word loss: 3.071042185244353\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.4261019878997407: per-word loss: 3.082574975663337\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.436905790838375: per-word loss: 3.4070129635967787\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.4477095937770095: per-word loss: 3.478950175372037\n",
      "\n",
      "\n",
      "   (FRAG (NP Mommy) .)\n",
      "epoch 1.4585133967156438: per-word loss: 3.984798252159822\n",
      "\n",
      "\n",
      "   (FRAG (NP microphone) ?)\n",
      "epoch 1.4693171996542782: per-word loss: 3.299430068212611\n",
      "\n",
      "\n",
      "   (FRAG (NP Adam) .)\n",
      "epoch 1.4801210025929128: per-word loss: 3.96440084139506\n",
      "\n",
      "\n",
      "   (FRAG (NP some master) .)\n",
      "epoch 1.4909248055315472: per-word loss: 3.9803175225958123\n",
      "\n",
      "\n",
      "   (S (NP you) (VP went) .)\n",
      "epoch 1.5017286084701815: per-word loss: 3.3891921434246126\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.512532411408816: per-word loss: 3.5858188703948377\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 1.5233362143474503: per-word loss: 3.4763748251158617\n",
      "\n",
      "   (INTJ alright)\n",
      "epoch 1.5341400172860846: per-word loss: 3.2305048059772803\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.5449438202247192: per-word loss: 3.0455018043518067\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.5557476231633536: per-word loss: 3.226490576502303\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.566551426101988: per-word loss: 3.183842360973358\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.5773552290406223: per-word loss: 3.5001042229788646\n",
      "\n",
      "\n",
      "   (FRAG (NP a cat) .)\n",
      "epoch 1.5881590319792567: per-word loss: 3.3709853705713306\n",
      "\n",
      "\n",
      "   (FRAG (NP peach) .)\n",
      "epoch 1.598962834917891: per-word loss: 2.8371766039867277\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.6097666378565254: per-word loss: 3.5989937843420567\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 1.62057044079516: per-word loss: 2.6757158279418944\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.6313742437337944: per-word loss: 3.415054908612879\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.6421780466724287: per-word loss: 3.8917408548552412\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.652981849611063: per-word loss: 2.8317377857259802\n",
      "\n",
      "\n",
      "   (FRAG (NP baseball University) .)\n",
      "epoch 1.6637856525496975: per-word loss: 3.739422676640172\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.6745894554883318: per-word loss: 3.6700301826000215\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.6853932584269662: per-word loss: 4.12879160712747\n",
      "\n",
      "   (INTJ surprise)\n",
      "epoch 1.6961970613656008: per-word loss: 3.834017838889021\n",
      "\n",
      "\n",
      "that want some coffee .\n",
      "that want some coffee .\n",
      "   (S (NP that) (VP want (NP some coffee)) .)\n",
      "epoch 1.7070008643042351: per-word loss: 3.8803919736486283\n",
      "\n",
      "\n",
      "   (FRAG (WHNP a what) ?)\n",
      "epoch 1.7178046672428695: per-word loss: 3.4183012888981748\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.7286084701815039: per-word loss: 3.103239738863278\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.7394122731201382: per-word loss: 4.198203314434398\n",
      "\n",
      "\n",
      "it said some macaroni .\n",
      "it said some macaroni .\n",
      "   (S (NP it) (VP said (NP some macaroni)) .)\n",
      "epoch 1.7502160760587726: per-word loss: 3.8430385376997056\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.7610198789974072: per-word loss: 3.2886965400294255\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.7718236819360416: per-word loss: 3.138015583941811\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.782627484874676: per-word loss: 3.868566943859232\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.7934312878133103: per-word loss: 3.2997282565109374\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.8042350907519447: per-word loss: 3.557541707356771\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.815038893690579: per-word loss: 3.920601080445682\n",
      "\n",
      "\n",
      "   (FRAG (NP il) ?)\n",
      "epoch 1.8258426966292134: per-word loss: 2.718198336459495\n",
      "\n",
      "\n",
      "   (FRAG (WHNP what else) ?)\n",
      "epoch 1.836646499567848: per-word loss: 3.5251385934891237\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.8474503025064823: per-word loss: 3.6737958847131944\n",
      "\n",
      "   (INTJ much-faster whoops .)\n",
      "epoch 1.8582541054451167: per-word loss: 4.0216583790986435\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.869057908383751: per-word loss: 3.2418742732725283\n",
      "\n",
      "\n",
      "   (FRAG (WHNP what) ?)\n",
      "epoch 1.8798617113223854: per-word loss: 3.7596251947416675\n",
      "\n",
      "\n",
      "   (FRAG (WHNP what) ?)\n",
      "epoch 1.8906655142610198: per-word loss: 3.6362326366222457\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.9014693171996542: per-word loss: 2.9864683602307296\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.9122731201382888: per-word loss: 3.63985270528651\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.9230769230769231: per-word loss: 3.2936691021759237\n",
      "\n",
      "\n",
      "   (FRAG (NP a bunny) .)\n",
      "epoch 1.9338807260155575: per-word loss: 3.9190872093987843\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 1.9446845289541919: per-word loss: 3.591884126265844\n",
      "\n",
      "   (INTJ here)\n",
      "epoch 1.9554883318928262: per-word loss: 4.272134760082168\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.9662921348314606: per-word loss: 2.8998439223678023\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 1.9770959377700952: per-word loss: 3.7279894307255743\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 1.9878997407087295: per-word loss: 3.5645181472946263\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 1.998703543647364: per-word loss: 3.2113294499985714\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 2.0095073465859983: per-word loss: 2.877474747799538\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.0203111495246326: per-word loss: 2.5644569827450647\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.031114952463267: per-word loss: 3.908678007797456\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.0419187554019014: per-word loss: 3.7304627187324293\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.0527225583405357: per-word loss: 3.2945818309366266\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.06352636127917: per-word loss: 3.611980361204881\n",
      "\n",
      "\n",
      "   (FRAG (NP dissimilar) , (NP ball) .)\n",
      "epoch 2.0743301642178045: per-word loss: 4.149836340371301\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.0851339671564393: per-word loss: 3.0132308249926045\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.0959377700950736: per-word loss: 3.9881698144109627\n",
      "\n",
      "   (INTJ ok)\n",
      "epoch 2.106741573033708: per-word loss: 3.4798362121153414\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.1175453759723424: per-word loss: 3.9013022917967577\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.1283491789109767: per-word loss: 4.691379403107918\n",
      "\n",
      "   (INTJ no .)\n",
      "epoch 2.139152981849611: per-word loss: 3.5181660585470134\n",
      "\n",
      "\n",
      "   (FRAG (NP tea) .)\n",
      "epoch 2.1499567847882455: per-word loss: 3.1751103124310895\n",
      "\n",
      "\n",
      "I take that out .\n",
      "I take that out .\n",
      "   (S (NP I) (VP take (NP that) (PRT out)) .)\n",
      "epoch 2.16076058772688: per-word loss: 2.7548675504671474\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.171564390665514: per-word loss: 3.4646165405000957\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.1823681936041486: per-word loss: 3.1192988642330826\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.193171996542783: per-word loss: 3.3527363965540755\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.2039757994814173: per-word loss: 3.199099412843502\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.2147796024200517: per-word loss: 3.725473559932944\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.225583405358686: per-word loss: 3.6068382120844142\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.236387208297321: per-word loss: 3.0446539172878513\n",
      "\n",
      "   (INTJ ok)\n",
      "epoch 2.247191011235955: per-word loss: 3.764003206969826\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.2579948141745896: per-word loss: 3.3195777871554957\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.268798617113224: per-word loss: 3.2571504233588633\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.2796024200518583: per-word loss: 3.216790707487809\n",
      "\n",
      "   (INTJ seasonnaly .)\n",
      "epoch 2.2904062229904927: per-word loss: 4.303556219527596\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.301210025929127: per-word loss: 2.7631322860717775\n",
      "\n",
      "   (INTJ 13th-biggest .)\n",
      "epoch 2.3120138288677614: per-word loss: 3.588104197649452\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.3228176318063958: per-word loss: 3.125421977212243\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 2.33362143474503: per-word loss: 4.302399290739185\n",
      "\n",
      "\n",
      "   (FRAG (NP some hot-water) ?)\n",
      "epoch 2.3444252376836645: per-word loss: 4.160034988544606\n",
      "\n",
      "\n",
      "that want shaving surprise .\n",
      "that want shaving surprise .\n",
      "   (S (NP that) (VP want (NP shaving surprise)) .)\n",
      "epoch 2.355229040622299: per-word loss: 4.188085416807746\n",
      "\n",
      "\n",
      "you do n't think you\n",
      "you do n't think you\n",
      "you do n't think you\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "you do n't think you can put a cup in\n",
      "   (S (NP you) (VP do n't (VP think (SBAR (S (NP you) (VP can (VP put (NP a cup) (PRT in))))))) .)\n",
      "epoch 2.3660328435609332: per-word loss: 3.4520922705933854\n",
      "\n",
      "   (INTJ no .)\n",
      "epoch 2.376836646499568: per-word loss: 2.9334272697790347\n",
      "\n",
      "   (INTJ no)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2.3876404494382024: per-word loss: 3.668893280192318\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.398444252376837: per-word loss: 3.087910641642178\n",
      "\n",
      "   (INTJ hello)\n",
      "epoch 2.409248055315471: per-word loss: 3.552797340484987\n",
      "\n",
      "\n",
      "   (S (VP look) ?)\n",
      "epoch 2.4200518582541055: per-word loss: 3.729003994670135\n",
      "\n",
      "   (INTJ no no)\n",
      "epoch 2.43085566119274: per-word loss: 3.379810722161692\n",
      "\n",
      "\n",
      "you do n't know .\n",
      "you do n't know .\n",
      "   (S (NP you) (VP do n't (VP know)) .)\n",
      "epoch 2.4416594641313742: per-word loss: 3.4281134104383164\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 2.4524632670700086: per-word loss: 3.3716699387653764\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.463267070008643: per-word loss: 3.9526384907799796\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.4740708729472773: per-word loss: 2.706306640058756\n",
      "\n",
      "   (INTJ whoops .)\n",
      "epoch 2.4848746758859117: per-word loss: 4.050569335619609\n",
      "\n",
      "   (INTJ goodbye .)\n",
      "epoch 2.495678478824546: per-word loss: 3.450463701344136\n",
      "\n",
      "\n",
      "   (FRAG (INTJ yes) .)\n",
      "epoch 2.5064822817631804: per-word loss: 3.2008837257112774\n",
      "\n",
      "   (INTJ hmm ?)\n",
      "epoch 2.5172860847018153: per-word loss: 3.5321860883220935\n",
      "\n",
      "\n",
      "   (FRAG (NP a nasality) .)\n",
      "epoch 2.5280898876404496: per-word loss: 3.375667391032198\n",
      "\n",
      "\n",
      "   (FRAG (WHNP what) ?)\n",
      "epoch 2.538893690579084: per-word loss: 3.4505606272541884\n",
      "\n",
      "\n",
      "   (FRAG (NP tricycle) .)\n",
      "epoch 2.5496974935177183: per-word loss: 3.319689356105428\n",
      "\n",
      "\n",
      "   (FRAG (NP a cereal) .)\n",
      "epoch 2.5605012964563527: per-word loss: 3.5732181732471173\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.571305099394987: per-word loss: 3.245136888933853\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.5821089023336214: per-word loss: 3.1965807419788987\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.592912705272256: per-word loss: 3.70102271327266\n",
      "\n",
      "\n",
      "   (FRAG (NP Adam) .)\n",
      "epoch 2.60371650821089: per-word loss: 3.3449167164278704\n",
      "\n",
      "   (INTJ alright)\n",
      "epoch 2.6145203111495245: per-word loss: 4.027893296603499\n",
      "\n",
      "   (INTJ alright .)\n",
      "epoch 2.625324114088159: per-word loss: 3.256090462613268\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.6361279170267933: per-word loss: 3.3673296098355894\n",
      "\n",
      "\n",
      "   (FRAG (NP it) ?)\n",
      "epoch 2.6469317199654276: per-word loss: 3.575762010755993\n",
      "\n",
      "\n",
      "I did n't see mussels\n",
      "I did n't see mussels\n",
      "I did n't see mussels\n",
      "I did n't see mussels\n",
      "   (S (NP I) (VP did n't (VP see (NP mussels))) .)\n",
      "epoch 2.657735522904062: per-word loss: 2.842628011703491\n",
      "\n",
      "\n",
      "   (S (NP I) (VP 're (VP going)) .)\n",
      "epoch 2.668539325842697: per-word loss: 4.304390900475639\n",
      "\n",
      "   (INTJ ok)\n",
      "epoch 2.679343128781331: per-word loss: 2.886252161897259\n",
      "\n",
      "   (INTJ surprise)\n",
      "epoch 2.6901469317199656: per-word loss: 2.861347275921422\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.7009507346586: per-word loss: 3.8132100850343704\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.7117545375972343: per-word loss: 3.332220714569092\n",
      "\n",
      "   (INTJ , not .)\n",
      "epoch 2.7225583405358686: per-word loss: 3.03283850859243\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.733362143474503: per-word loss: 3.397624385821355\n",
      "\n",
      "   (INTJ per-mile .)\n",
      "epoch 2.7441659464131374: per-word loss: 3.411296776362828\n",
      "\n",
      "\n",
      "   (S (NP you) (VP have (NP clay)) .)\n",
      "epoch 2.7549697493517717: per-word loss: 3.5441761717096076\n",
      "\n",
      "\n",
      "   (S (NP that) (VP 's (ADJP dirty)) ?)\n",
      "epoch 2.765773552290406: per-word loss: 4.009783019758251\n",
      "\n",
      "   (INTJ well)\n",
      "epoch 2.7765773552290405: per-word loss: 3.1279215029069594\n",
      "\n",
      "   (INTJ ok)\n",
      "epoch 2.787381158167675: per-word loss: 3.674010473531443\n",
      "\n",
      "\n",
      "   (FRAG (NP I) .)\n",
      "epoch 2.798184961106309: per-word loss: 3.2911483380529614\n",
      "\n",
      "\n",
      "   (FRAG (NP cage) .)\n",
      "epoch 2.808988764044944: per-word loss: 2.907456415936463\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.8197925669835784: per-word loss: 3.314653342497264\n",
      "\n",
      "   (INTJ no)\n",
      "epoch 2.8305963699222128: per-word loss: 4.183206321337284\n",
      "\n",
      "   (INTJ no .)\n",
      "epoch 2.841400172860847: per-word loss: 3.557990802418102\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.8522039757994815: per-word loss: 3.6631557561789347\n",
      "\n",
      "\n",
      "you do n't know .\n",
      "you do n't know .\n",
      "   (S (NP you) (VP do n't (VP know)) .)\n",
      "epoch 2.863007778738116: per-word loss: 2.929061961623858\n",
      "\n",
      "   (INTJ whoops .)\n",
      "epoch 2.87381158167675: per-word loss: 3.0535530927297954\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.8846153846153846: per-word loss: 3.8559930356556937\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.895419187554019: per-word loss: 2.943577783559662\n",
      "\n",
      "   (INTJ yes .)\n",
      "epoch 2.9062229904926533: per-word loss: 2.898300493374849\n",
      "\n",
      "   (INTJ please .)\n",
      "epoch 2.9170267934312877: per-word loss: 3.2112065057684904\n",
      "\n",
      "   (INTJ no .)\n",
      "epoch 2.927830596369922: per-word loss: 3.7286752382914226\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.9386343993085564: per-word loss: 2.747616670285052\n",
      "\n",
      "   (INTJ oh)\n",
      "epoch 2.949438202247191: per-word loss: 3.8388326079757125\n",
      "\n",
      "   (INTJ alright .)\n",
      "epoch 2.9602420051858256: per-word loss: 3.600576445863054\n",
      "\n",
      "   (INTJ oh hi)\n",
      "epoch 2.97104580812446: per-word loss: 3.70349349264513\n",
      "\n",
      "\n",
      "   (FRAG (NP boat) .)\n",
      "epoch 2.9818496110630943: per-word loss: 3.006084359354443\n",
      "\n",
      "   (INTJ yes)\n",
      "epoch 2.9926534140017287: per-word loss: 4.183500670675022\n",
      "\n",
      "\n",
      "   (FRAG (NP queen) ?)\n",
      "Done saving model!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"lets go\")\n",
    "train, dev, test = load_data(train_file, dev_file, test_file)\n",
    "print(\"done loading files\")\n",
    "vocab_acts = create_vocab([x[2] for x in train])\n",
    "print(\"loading base model params\")\n",
    "model = dy.ParameterCollection()\n",
    "print(\"done loading param graph\")\n",
    "tp = TransitionParser(model, cluster_file, vocab_acts)\n",
    "print(\"loading model\")\n",
    "model.populate(\"epoch11.model\")\n",
    "print(\"done loading model\")\n",
    "# losses, results = tp.train(train, dy.SimpleSGDTrainer(model), dev, epochs=3)\n",
    "# model.populate(\"shabba.model\")\n",
    "model.save(\"epoch11.model\")\n",
    "print(\"Done saving model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abandoned-homework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.310749053955078, 7.5704240122149065, 4.306818440444488, 5.175891101837158, 5.9765869073378735, 4.8175116740740265, 4.876853331923485, 5.440113775039974, 4.656709229151408, 4.648396356731442, 4.7237237023969065, 3.82617195977105, 4.39092510396784, 3.893747930239914, 4.651157840092977, 4.836745023727417, 4.432012681719623, 4.487669766282236, 5.073818479735276, 4.922428687413533, 3.8695394022124154, 4.57090816622466, 4.357139234125179, 3.8868870735168457, 3.860529268887026, 4.198434967041016, 3.7908260369602638, 3.643617996080654, 3.8717567814721003, 3.3840293384887077, 4.811170435571051, 4.086638779174991, 4.217640302902044, 3.814608630600509, 3.6164834153267646, 4.023435288005405, 4.844790556214073, 4.086725532192073, 3.5339399123376656, 4.081922294663602, 3.731634592126917, 3.966839241259026, 4.271126059001078, 4.40050806586198, 3.3222726415271406, 4.33611555840518, 4.113950344356331, 4.4647652990866025, 4.755603873729706, 4.871568429881129, 4.195627617835998, 3.378002807497978, 3.9619275816025272, 3.7214300846231394, 3.6283928417577975, 4.292386353810628, 4.72835774179818, 4.357748922374514, 4.491729819256326, 4.36438123478609, 3.385259889421009, 3.8775943463498894, 2.7735102488324532, 5.046516490269856, 3.531767146688112, 4.399376803596548, 2.7384557287201625, 4.011950678529993, 4.108599496610237, 3.618754958794787, 4.265545726667905, 4.002274027769117, 3.889943582910887, 3.976684497881539, 4.096668903994712, 3.488561481725974, 3.9126011009874015, 3.4213343411684036, 3.5152560021426225, 4.775047930511269, 4.265729624754305, 3.9515680966976876, 4.088831845257017, 3.1495389520076285, 3.178407877878426, 3.9764681422349177, 3.6363779107729592, 4.83605893202654, 4.329062640211965, 3.133599383690778, 4.5593295130931155, 3.8522115177578398, 4.492373304429397, 4.032533403100638, 2.759404429188975, 3.675924848765135, 3.4155947053936164, 3.9852742119658764, 3.024623158905241, 3.9309476912021637, 3.9033728094661937, 3.590115566381672, 3.4870376969776014, 4.086653783723905, 3.9403827869339496, 4.005422641491068, 3.5123180393129587, 3.910077383192323, 3.4701238285963703, 4.006687773797745, 3.826869253759031, 3.263009923825161, 3.4017996200143474, 3.3723682466941542, 4.523498915337227, 3.4016763300135517, 3.5602058470249176, 4.953028672120788, 3.869492976418857, 3.0220759220612354, 4.933207349777222, 3.6657838442968944, 3.3980517387390137, 3.2196120859562667, 3.486700248718262, 3.8957977034151554, 3.49120112026439, 4.122286879631781, 4.257029090853904, 3.95838262766776, 4.568411073019338, 3.8608913746009876, 3.2689270565384314, 3.60369134087094, 3.765269476846354, 3.2058101892471313, 4.798249285148851, 4.612913031327097, 3.453306902879439, 3.822924815433126, 3.7009671903124044, 4.3578668158987295, 4.013612043222732, 3.8322338814948016, 3.6557981151424044, 4.072547737432986, 3.891694041147624, 3.7980913012775024, 4.169210147857666, 3.7217281490564345, 3.7743965810345066, 3.9838729124320182, 4.213593636372293, 3.8926416709099287, 3.7244219132411627, 4.408652540566265, 4.3751698131403645, 4.202524538753795, 4.373581744254904, 4.193492107254138, 3.716879486141348, 3.5498716750112522, 3.557728412823799, 3.9765745460009967, 3.9319859560388717, 4.971627716986549, 4.255296969817857, 3.5978325137385614, 4.132960288755355, 4.0877099883729135, 3.5970562476261407, 4.0942066015786684, 3.299349985049881, 3.3396032735949657, 3.331381177290892, 4.072034532373602, 3.546564049190945, 3.758274638489501, 4.423984407439945, 3.7152109761391916, 3.4214414736581227, 3.6458092252892182, 4.316880811190774, 4.328765945081358, 3.950222619374593, 3.1523798555135727, 2.5486560786092602, 3.8937622279655644, 4.160418247613381, 3.333689898413581, 3.807230105647793, 3.158713623993379, 3.1043067836761473, 3.616100945965997, 3.921413609679316, 4.163060827860757, 3.458449649059866, 3.8314786710237203, 4.584638625383377, 4.194726094034792, 3.707212622736541, 3.462606833415961, 3.225393700751529, 3.75883174081992, 3.9891520911141445, 3.188227449791341, 3.174139256477356, 4.047922579778565, 3.536093922827741, 3.808103261455413, 3.630443505550686, 3.5865876095317235, 3.6827809247103604, 3.927507546769471, 4.277017632151038, 3.40304491519928, 4.041062235053069, 3.212085236443414, 3.7512895266215005, 4.164691473077411, 3.8184716926784965, 4.2156313711597075, 3.2484683273430157, 3.999537793365685, 3.2119761613699107, 4.084419616306101, 3.15430954449317, 3.4332745829715003, 3.67519751093746, 3.94141450929053, 3.3537247130211365, 3.1007219727631585, 3.8401439205096786, 3.8326490589550564, 3.1871979236602783, 3.362667173534245, 3.2503225421905517, 4.178964740889413, 3.7179705120666684, 3.491596348892302, 4.028384585129587, 4.509170639682823, 3.7519686937332155, 3.4766696414282157, 3.733062360904835, 3.6994689892078267, 3.651370332354591, 4.024578017910032, 3.83455710278617, 3.9068571117752833, 4.0475623200579385, 3.6565193784409673, 3.1739240588998436, 3.095630371017961, 4.186692351200541, 3.387440555974057, 3.3255452259330993, 3.2942734724320704, 3.3852671290473113, 3.7635394838121203, 4.052356169205304, 3.721320427954197, 4.282102697236198, 3.9802519536157797, 3.701939843251155, 4.2107022089987804, 3.2120768082369664, 3.897007490842397, 4.106627875000891, 3.279541761644425, 4.344989170301829, 4.045439600061488, 3.6719313296111853, 3.6929147572352967, 3.833417473612605, 4.283525591311247, 3.790241053004465, 3.731122271767978, 3.4422242641448975, 3.47006719074552, 3.599647293027663, 3.9015098839998243, 3.9874326501573836, 3.5470473098754884, 3.2300968264278613, 3.582009514436027, 3.643854120844289, 3.986881376195837, 3.7850933288460347, 3.68049428126956, 3.8781651706560285, 2.824750065803528, 4.017046373207252, 2.7262334480124006, 3.3407157812799726, 3.8042033802379263, 4.271933389015687, 3.5846750456711343, 4.33113034680593, 3.8565493037429035, 3.0570366924459282, 3.9016680702841353, 3.409250037114423, 4.114168158475904, 3.664382481037226, 3.4486085627530074, 3.220987765412582, 3.60854443194161, 3.959960464221328, 3.724479206565286, 3.294669425940212, 2.9329342714092075, 4.410916097702518, 3.5033290882905326, 3.8758887488266516, 3.858473304199846, 3.0976511811007974, 2.960852632586588, 3.6965211037989265, 2.497726369673206, 3.778422083591391, 2.822954993247986, 4.1295162374026155, 3.004651388112646, 4.080164651185768, 2.618384945924115, 3.843007792745318, 3.92074696163633, 3.087774462170071, 3.4409637279647716, 3.7284951134333535, 3.5424115494506, 3.997362502415975, 3.920258427319461, 3.8475544744258303, 3.9087682637301358, 3.594586621312534, 3.6927563419824914, 3.306808528521203, 3.2092748318078383, 3.0644976389493874, 3.480475713224972, 3.5892142924395474, 3.666656685202089, 3.539074943179176, 3.3962484705234, 4.20533702547187, 4.109046639863008, 4.10751465764539, 3.956614038612269, 3.5141405393095577, 3.5978503101012285, 3.052660919004871, 3.618855949882027, 4.014275795883602, 3.651511499847191, 3.0355941513005424, 3.768723490834236, 3.957059891687499, 2.830167683688077, 3.179273346801857, 3.6873631337109734, 3.6037408835451368, 3.122245801612735, 3.417932117222161, 4.306057296260711, 3.3928253650665283, 4.33597354768957, 3.37210472571997, 3.603920201997499, 4.112675906708522, 3.050766185178595, 4.3637807386892815, 3.573961084171877, 3.376491652594672, 3.171206058376897, 3.874794307028412, 3.5500244371818774, 2.911850757598877, 3.66585729586198, 4.572780081944436, 2.627832167076342, 2.8807981862317797, 2.9494229485006893, 4.080800286766623, 3.2725572727344656, 3.155789638732697, 3.7221574584643045, 3.570701623367051, 4.344724627306862, 4.341984344691765, 2.763315718873103, 3.425968493063619, 3.1548727716718403, 3.7913914438503893, 4.694350541003643, 3.795222661266588, 3.114927221660965, 3.5843183233382856, 3.839085828873419, 4.12238135843566, 2.8461393390012826, 4.250650857855206, 3.438610989706857, 3.672200613473294, 3.3748973205055988, 3.4483617598360237, 3.31462662092602, 3.2403033586937613, 3.071042185244353, 3.082574975663337, 3.4070129635967787, 3.478950175372037, 3.984798252159822, 3.299430068212611, 3.96440084139506, 3.9803175225958123, 3.3891921434246126, 3.5858188703948377, 3.4763748251158617, 3.2305048059772803, 3.0455018043518067, 3.226490576502303, 3.183842360973358, 3.5001042229788646, 3.3709853705713306, 2.8371766039867277, 3.5989937843420567, 2.6757158279418944, 3.415054908612879, 3.8917408548552412, 2.8317377857259802, 3.739422676640172, 3.6700301826000215, 4.12879160712747, 3.834017838889021, 3.8803919736486283, 3.4183012888981748, 3.103239738863278, 4.198203314434398, 3.8430385376997056, 3.2886965400294255, 3.138015583941811, 3.868566943859232, 3.2997282565109374, 3.557541707356771, 3.920601080445682, 2.718198336459495, 3.5251385934891237, 3.6737958847131944, 4.0216583790986435, 3.2418742732725283, 3.7596251947416675, 3.6362326366222457, 2.9864683602307296, 3.63985270528651, 3.2936691021759237, 3.9190872093987843, 3.591884126265844, 4.272134760082168, 2.8998439223678023, 3.7279894307255743, 3.5645181472946263, 3.2113294499985714, 2.877474747799538, 2.5644569827450647, 3.908678007797456, 3.7304627187324293, 3.2945818309366266, 3.611980361204881, 4.149836340371301, 3.0132308249926045, 3.9881698144109627, 3.4798362121153414, 3.9013022917967577, 4.691379403107918, 3.5181660585470134, 3.1751103124310895, 2.7548675504671474, 3.4646165405000957, 3.1192988642330826, 3.3527363965540755, 3.199099412843502, 3.725473559932944, 3.6068382120844142, 3.0446539172878513, 3.764003206969826, 3.3195777871554957, 3.2571504233588633, 3.216790707487809, 4.303556219527596, 2.7631322860717775, 3.588104197649452, 3.125421977212243, 4.302399290739185, 4.160034988544606, 4.188085416807746, 3.4520922705933854, 2.9334272697790347, 3.668893280192318, 3.087910641642178, 3.552797340484987, 3.729003994670135, 3.379810722161692, 3.4281134104383164, 3.3716699387653764, 3.9526384907799796, 2.706306640058756, 4.050569335619609, 3.450463701344136, 3.2008837257112774, 3.5321860883220935, 3.375667391032198, 3.4505606272541884, 3.319689356105428, 3.5732181732471173, 3.245136888933853, 3.1965807419788987, 3.70102271327266, 3.3449167164278704, 4.027893296603499, 3.256090462613268, 3.3673296098355894, 3.575762010755993, 2.842628011703491, 4.304390900475639, 2.886252161897259, 2.861347275921422, 3.8132100850343704, 3.332220714569092, 3.03283850859243, 3.397624385821355, 3.411296776362828, 3.5441761717096076, 4.009783019758251, 3.1279215029069594, 3.674010473531443, 3.2911483380529614, 2.907456415936463, 3.314653342497264, 4.183206321337284, 3.557990802418102, 3.6631557561789347, 2.929061961623858, 3.0535530927297954, 3.8559930356556937, 2.943577783559662, 2.898300493374849, 3.2112065057684904, 3.7286752382914226, 2.747616670285052, 3.8388326079757125, 3.600576445863054, 3.70349349264513, 3.006084359354443, 4.183500670675022]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "prev_loss = [16.310749053955078, 7.5704240122149065, 4.306818440444488, 5.175891101837158, 5.9765869073378735, 4.8175116740740265, 4.876853331923485, 5.440113775039974, 4.656709229151408, 4.648396356731442, 4.7237237023969065, 3.82617195977105, 4.39092510396784, 3.893747930239914, 4.651157840092977, 4.836745023727417, 4.432012681719623, 4.487669766282236, 5.073818479735276, 4.922428687413533, 3.8695394022124154, 4.57090816622466, 4.357139234125179, 3.8868870735168457, 3.860529268887026, 4.198434967041016, 3.7908260369602638, 3.643617996080654, 3.8717567814721003, 3.3840293384887077, 4.811170435571051, 4.086638779174991, 4.217640302902044, 3.814608630600509, 3.6164834153267646, 4.023435288005405, 4.844790556214073, 4.086725532192073, 3.5339399123376656, 4.081922294663602, 3.731634592126917, 3.966839241259026, 4.271126059001078, 4.40050806586198, 3.3222726415271406, 4.33611555840518, 4.113950344356331, 4.4647652990866025, 4.755603873729706, 4.871568429881129, 4.195627617835998, 3.378002807497978, 3.9619275816025272, 3.7214300846231394, 3.6283928417577975, 4.292386353810628, 4.72835774179818, 4.357748922374514, 4.491729819256326, 4.36438123478609, 3.385259889421009, 3.8775943463498894, 2.7735102488324532, 5.046516490269856, 3.531767146688112, 4.399376803596548, 2.7384557287201625, 4.011950678529993, 4.108599496610237, 3.618754958794787, 4.265545726667905, 4.002274027769117, 3.889943582910887, 3.976684497881539, 4.096668903994712, 3.488561481725974, 3.9126011009874015, 3.4213343411684036, 3.5152560021426225, 4.775047930511269, 4.265729624754305, 3.9515680966976876, 4.088831845257017, 3.1495389520076285, 3.178407877878426, 3.9764681422349177, 3.6363779107729592, 4.83605893202654, 4.329062640211965, 3.133599383690778, 4.5593295130931155, 3.8522115177578398, 4.492373304429397, 4.032533403100638, 2.759404429188975, 3.675924848765135, 3.4155947053936164, 3.9852742119658764, 3.024623158905241, 3.9309476912021637, 3.9033728094661937, 3.590115566381672, 3.4870376969776014, 4.086653783723905, 3.9403827869339496, 4.005422641491068, 3.5123180393129587, 3.910077383192323, 3.4701238285963703, 4.006687773797745, 3.826869253759031, 3.263009923825161, 3.4017996200143474, 3.3723682466941542, 4.523498915337227, 3.4016763300135517, 3.5602058470249176, 4.953028672120788, 3.869492976418857, 3.0220759220612354, 4.933207349777222, 3.6657838442968944, 3.3980517387390137, 3.2196120859562667, 3.486700248718262, 3.8957977034151554, 3.49120112026439, 4.122286879631781, 4.257029090853904, 3.95838262766776, 4.568411073019338, 3.8608913746009876, 3.2689270565384314, 3.60369134087094, 3.765269476846354, 3.2058101892471313, 4.798249285148851, 4.612913031327097, 3.453306902879439, 3.822924815433126, 3.7009671903124044, 4.3578668158987295, 4.013612043222732, 3.8322338814948016, 3.6557981151424044, 4.072547737432986, 3.891694041147624, 3.7980913012775024, 4.169210147857666, 3.7217281490564345, 3.7743965810345066, 3.9838729124320182, 4.213593636372293, 3.8926416709099287, 3.7244219132411627, 4.408652540566265, 4.3751698131403645, 4.202524538753795, 4.373581744254904, 4.193492107254138, 3.716879486141348, 3.5498716750112522, 3.557728412823799, 3.9765745460009967, 3.9319859560388717, 4.971627716986549, 4.255296969817857, 3.5978325137385614, 4.132960288755355, 4.0877099883729135, 3.5970562476261407, 4.0942066015786684, 3.299349985049881, 3.3396032735949657, 3.331381177290892, 4.072034532373602, 3.546564049190945, 3.758274638489501, 4.423984407439945, 3.7152109761391916, 3.4214414736581227, 3.6458092252892182, 4.316880811190774, 4.328765945081358, 3.950222619374593, 3.1523798555135727, 2.5486560786092602, 3.8937622279655644, 4.160418247613381, 3.333689898413581, 3.807230105647793, 3.158713623993379, 3.1043067836761473, 3.616100945965997, 3.921413609679316, 4.163060827860757, 3.458449649059866, 3.8314786710237203, 4.584638625383377, 4.194726094034792, 3.707212622736541, 3.462606833415961, 3.225393700751529, 3.75883174081992, 3.9891520911141445, 3.188227449791341, 3.174139256477356, 4.047922579778565, 3.536093922827741, 3.808103261455413, 3.630443505550686, 3.5865876095317235, 3.6827809247103604, 3.927507546769471, 4.277017632151038, 3.40304491519928, 4.041062235053069, 3.212085236443414, 3.7512895266215005, 4.164691473077411, 3.8184716926784965, 4.2156313711597075, 3.2484683273430157, 3.999537793365685, 3.2119761613699107, 4.084419616306101, 3.15430954449317, 3.4332745829715003, 3.67519751093746, 3.94141450929053, 3.3537247130211365, 3.1007219727631585, 3.8401439205096786, 3.8326490589550564, 3.1871979236602783, 3.362667173534245, 3.2503225421905517, 4.178964740889413, 3.7179705120666684, 3.491596348892302, 4.028384585129587, 4.509170639682823, 3.7519686937332155, 3.4766696414282157, 3.733062360904835, 3.6994689892078267, 3.651370332354591, 4.024578017910032, 3.83455710278617, 3.9068571117752833, 4.0475623200579385, 3.6565193784409673, 3.1739240588998436, 3.095630371017961, 4.186692351200541, 3.387440555974057, 3.3255452259330993, 3.2942734724320704, 3.3852671290473113, 3.7635394838121203, 4.052356169205304, 3.721320427954197, 4.282102697236198, 3.9802519536157797, 3.701939843251155, 4.2107022089987804, 3.2120768082369664, 3.897007490842397, 4.106627875000891, 3.279541761644425, 4.344989170301829, 4.045439600061488, 3.6719313296111853, 3.6929147572352967, 3.833417473612605, 4.283525591311247, 3.790241053004465, 3.731122271767978]\n",
    "epoch11_losses = prev_loss + losses\n",
    "fig = plt.figure()\n",
    "plt.plot([i for i in range(len(epoch11_losses))], epoch11_losses)\n",
    "fig.suptitle(\"Per-Word Log Loss\")\n",
    "plt.xlabel(\"Batched Words (25 per batch)\")\n",
    "plt.ylabel(\"Log Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "disciplinary-belize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP you) (VP do n't (VP know)) .)\n",
      "(S (NP he) (VP 's (ADJP right)) .)\n",
      "(S (NP this) (VP are (VP crossing (NP the box))) ?)\n",
      "(FRAG (NP baseball University) .)\n",
      "(S (NP that) (VP want (NP some coffee)) .)\n",
      "(S (NP it) (VP said (NP some macaroni)) .)\n",
      "(INTJ much-faster whoops .)\n",
      "(FRAG (NP dissimilar) , (NP ball) .)\n",
      "(S (NP I) (VP take (NP that) (PRT out)) .)\n",
      "(FRAG (NP some hot-water) ?)\n",
      "(S (NP that) (VP want (NP shaving surprise)) .)\n",
      "(S (NP you) (VP do n't (VP think (SBAR (S (NP you) (VP can (VP put (NP a cup) (PRT in))))))) .)\n",
      "(S (NP you) (VP do n't (VP know)) .)\n",
      "(S (NP I) (VP did n't (VP see (NP mussels))) .)\n",
      "(S (NP I) (VP 're (VP going)) .)\n",
      "(S (NP you) (VP have (NP clay)) .)\n",
      "(S (NP that) (VP 's (ADJP dirty)) ?)\n",
      "(S (NP you) (VP do n't (VP know)) .)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "curious-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NT(S)', 'NT(VP)', 'SHIFT', 'NT(SBAR)', 'NT(WHNP)', 'SHIFT', 'REDUCE', 'NT(S)', 'NT(VP)', 'SHIFT', 'NT(ADVP)', 'SHIFT', 'REDUCE', 'REDUCE', 'REDUCE', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (VP see (SBAR (WHNP what) (S (VP 's (ADVP inside))))) .)\n",
      "6.4827423095703125\n",
      "['NT(S)', 'NT(VP)', 'SHIFT', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(NP)', 'SHIFT', 'SHIFT', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (VP is (NP that) (NP his name)) ?)\n",
      "3.3970584869384766\n",
      "['NT(INTJ)', 'SHIFT', 'REDUCE', 'SHIFT', 'NT(FRAG)', 'NT(NP)', 'SHIFT', 'SHIFT', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(INTJ hello)\n",
      "1.031151008605957\n",
      "['NT(SBAR)', 'NT(WHADVP)', 'SHIFT', 'REDUCE', 'NT(S)', 'NT(VP)', 'SHIFT', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(ADVP)', 'SHIFT', 'REDUCE', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(SBAR (WHADVP how) (S (VP are (NP you) (ADVP today))) ?)\n",
      "4.040924072265625\n",
      "['NT(SBAR)', 'NT(WHNP)', 'SHIFT', 'REDUCE', 'NT(S)', 'NT(VP)', 'SHIFT', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(VP)', 'SHIFT', 'REDUCE', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(SBAR (WHNP what) (S (VP did (NP he) (VP do))) ?)\n",
      "2.5653451919555663\n",
      "['NT(S)', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(VP)', 'SHIFT', 'NT(NP)', 'SHIFT', 'SHIFT', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (NP he) (VP threw (NP some pie)) ?)\n",
      "4.7788818359375\n",
      "['NT(S)', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(VP)', 'SHIFT', 'NT(NP)', 'SHIFT', 'SHIFT', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (NP I) (VP have (NP some more)) .)\n",
      "3.368545150756836\n",
      "['NT(FRAG)', 'NT(VP)', 'SHIFT', 'NT(VP)', 'SHIFT', 'NT(S)', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(ADJP)', 'SHIFT', 'REDUCE', 'REDUCE', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(FRAG (VP to (VP keep (S (NP it) (ADJP closed)))) .)\n",
      "7.965965270996094\n",
      "['NT(S)', 'NT(NP)', 'SHIFT', 'REDUCE', 'NT(VP)', 'SHIFT', 'NT(ADJP)', 'SHIFT', 'SHIFT', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (NP it) (VP is (ADJP very sharp)) .)\n",
      "4.573316192626953\n",
      "['NT(S)', 'NT(VP)', 'SHIFT', 'NT(VP)', 'SHIFT', 'NT(WHNP)', 'SHIFT', 'SHIFT', 'REDUCE', 'REDUCE', 'REDUCE', 'SHIFT', 'REDUCE']\n",
      "(S (VP to (VP make (WHNP a what))) ?)\n",
      "4.888454818725586\n"
     ]
    }
   ],
   "source": [
    "for (_, s, a) in train[1000:1010]:\n",
    "    result, loss = tp.generate(s, a, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "professional-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.591711072921753\n",
      "4.15346061706543\n",
      "4.532235155105591\n",
      "4.761990337371826\n",
      "5.717469486823449\n",
      "6.261814101537069\n",
      "6.3433059851328535\n",
      "5.234972131466437\n",
      "4.952698490619659\n",
      "4.015413823127747\n",
      "3.3356433081626893\n",
      "5.252345931529999\n",
      "4.559257632161414\n",
      "4.290288427352905\n",
      "4.78341236114502\n",
      "4.615433944702149\n",
      "4.513944219589233\n",
      "5.394028778076172\n",
      "4.905654796926152\n",
      "4.640450263023377\n",
      "4.146114813486735\n",
      "4.635127873420715\n",
      "4.526238568623861\n",
      "4.442049902961368\n",
      "4.078441606249128\n",
      "4.662068770272391\n",
      "3.8294444002423966\n",
      "4.283405095509121\n",
      "4.260055736619599\n",
      "4.295467768907547\n",
      "4.161488753557205\n",
      "4.755037401957685\n",
      "3.7815959241655137\n",
      "4.2198015138838025\n",
      "3.43886921955989\n",
      "3.9741594256945754\n",
      "4.588756031843465\n",
      "4.766299750374966\n",
      "5.105603967903263\n"
     ]
    }
   ],
   "source": [
    "tp.test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behind-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NT(WHADJP)', 1: 'NT(NP)', 2: 'NT(PRT)', 3: 'REDUCE', 4: 'NT(ADVP)', 5: 'NT(INTJ)', 6: 'NT(ADJP)', 7: 'NT(WHPP)', 8: 'NT(FRAG)', 9: 'NT(S)', 10: 'NT(VP)', 11: 'NT(WHNP)', 12: 'SHIFT', 13: 'NT(WHADVP)', 14: 'NT(SBAR)', 15: 'NT(PP)'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_acts.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NT(WHADJP)': 0, 'NT(NP)': 1, 'NT(PRT)': 2, 'REDUCE': 3, 'NT(ADVP)': 4, 'NT(INTJ)': 5, 'NT(ADJP)': 6, 'NT(WHPP)': 7, 'NT(FRAG)': 8, 'NT(S)': 9, 'NT(VP)': 10, 'NT(WHNP)': 11, 'SHIFT': 12, 'NT(WHADVP)': 13, 'NT(SBAR)': 14, 'NT(PP)': 15}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_acts.w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "little-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316710\n"
     ]
    }
   ],
   "source": [
    "print(tp.vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "otherwise-discretion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1889 96\n"
     ]
    }
   ],
   "source": [
    "# This cell computes overlap in words in Adam corpus to Brown cluster\n",
    "words_map = {}\n",
    "corpus = train + test + dev\n",
    "for (_, s, _) in corpus:\n",
    "    for word in s:\n",
    "        if word in tp.vocab.w2i:\n",
    "            words_map[word] = 1\n",
    "        else:\n",
    "            words_map[word] = -1\n",
    "in_word, out_word = 0, 0\n",
    "for word in words_map:\n",
    "    if words_map[word] == 1:\n",
    "        in_word += 1\n",
    "    else:\n",
    "        out_word += 1\n",
    "print(in_word, out_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
